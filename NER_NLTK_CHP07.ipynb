{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-NLTK-CHP07.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "_VTmHIjvrDE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implementing Named Entity Recognition Using NLTK CHP07"
      ]
    },
    {
      "metadata": {
        "id": "790iKMjUrANv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "61560710-00de-4d4c-f2a5-0b2e378f0011"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z63m7I1s5KwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paragraph is first broken into sentences using \"sent_tokenize\", then sentence is broken into words using \"word_tokenize\" and finally POS tags of each word is found\n",
        "def ie_preprocess(document):\n",
        "  sentences = nltk.sent_tokenize(document) \n",
        "  sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "  sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
        "  return(sentences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBiHxHR35YxA",
        "colab_type": "code",
        "outputId": "d3d3ef61-bfa0-458b-ad8d-8e8c68b2b9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sentence = ie_preprocess('the little dog barked at the cat')\n",
        "print(sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('the', 'DT'), ('little', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nkE4GPbY6Ly-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b2238bbe-a1dc-46d8-8b0c-a353323a5c12"
      },
      "cell_type": "code",
      "source": [
        "# Defining grammar to chunk the sentence\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar) \n",
        "result = cp.parse(sentence[0])\n",
        "print('chunked sentence', result)\n",
        "print('data type of result', type(result))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chunked sentence (S (NP the/DT little/JJ dog/NN) barked/VBD at/IN (NP the/DT cat/NN))\n",
            "data type of result <class 'nltk.tree.Tree'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ccH_vzhZ60ZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N-grams - are continous sequence of n-items in a sentence. Depending on the input n, the following function returns n-grams of the sentence.\n",
        "\n",
        "# Implementing N-grams using Regex\n",
        "def generate_ngrams(s, n):\n",
        "    # Convert to lowercases\n",
        "    s = s.lower()\n",
        "    \n",
        "    # Replace all none alphanumeric characters with spaces\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "    \n",
        "    # Break sentence in the token, remove empty tokens\n",
        "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "    \n",
        "    # Use the zip function to help us generate n-grams\n",
        "    # Concatentate the tokens into ngrams and return\n",
        "    print([tokens[:i] for i in range(n)])\n",
        "#     ngrams = zip(*[tokens[:i] for i in range(n)])\n",
        "#     return([\" \".join(ngram) for ngram in ngrams])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFO73-A863vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a85c89b7-63fb-49d2-e6dd-01fb578eb473"
      },
      "cell_type": "code",
      "source": [
        "s = \"Existing Natural Language Techniques (NLP) focus mostly on transcribing what humans say, rather than understanding what’s being said. Even with the release of advanced chatbot technologies like Google Duplex and Microsoft’s Xiaoice, this is a challenge that has eluded researchers so far.\"\n",
        "generate_ngrams(s, 3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], ['existing'], ['existing', 'natural']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CVjTcDJztilC",
        "colab_type": "code",
        "outputId": "3df9435a-ef99-4e88-bca2-580a995eda4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Implementing N-grams using NLTK\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "s = s.lower()\n",
        "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
        "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
        "output = list(ngrams(tokens, 2))\n",
        "print(output)\n",
        "print(len(output),len(s))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('existing', 'natural'), ('natural', 'language'), ('language', 'techniques'), ('techniques', 'nlp'), ('nlp', 'focus'), ('focus', 'mostly'), ('mostly', 'on'), ('on', 'transcribing'), ('transcribing', 'what'), ('what', 'humans'), ('humans', 'say'), ('say', 'rather'), ('rather', 'than'), ('than', 'understanding'), ('understanding', 'what'), ('what', 's'), ('s', 'being'), ('being', 'said'), ('said', 'even'), ('even', 'with'), ('with', 'the'), ('the', 'release'), ('release', 'of'), ('of', 'advanced'), ('advanced', 'chatbot'), ('chatbot', 'technologies'), ('technologies', 'like'), ('like', 'google'), ('google', 'duplex'), ('duplex', 'and'), ('and', 'microsoft'), ('microsoft', 's'), ('s', 'xiaoice'), ('xiaoice', 'this'), ('this', 'is'), ('is', 'a'), ('a', 'challenge'), ('challenge', 'that'), ('that', 'has'), ('has', 'eluded'), ('eluded', 'researchers'), ('researchers', 'so'), ('so', 'far')]\n",
            "43 288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXIi9Ua_vKU-",
        "colab_type": "code",
        "outputId": "7fa04e0f-bf19-474b-a7dd-3043646761d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('conll2000')\n",
        "from nltk.corpus import conll2000\n",
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "    def __init__(self, train_sents):\n",
        "      train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]                                   \n",
        "                    for sent in train_sents]\n",
        "      self.tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "      pos_tags = [pos for (word,pos) in sentence]\n",
        "      tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "      chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "      conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
        "                   in zip(sentence, chunktags)]\n",
        "      return nltk.chunk.conlltags2tree(conlltags)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXYI6oJMwYuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d7a267ee-d226-49e1-f7ae-e5130448da6e"
      },
      "cell_type": "code",
      "source": [
        "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
        "unigram_chunker = UnigramChunker(train_sents)\n",
        "print(unigram_chunker.evaluate(test_sents))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSjQhsXEvqnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}